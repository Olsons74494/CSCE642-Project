{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d298ed-252c-438e-aa05-5bd1a728c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb0b340b-fed1-465e-b8d7-b71846520cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VineyardSurrogate:\n",
    "    def __init__(self, grid_rows=16, grid_cols=12, et_coeff=0.05,\n",
    "                 optimal_low=0.35, optimal_high=0.75, stages_duration=50,\n",
    "                 end_on_death=True, diffusion=False, rand_et=True,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize the vineyard surrogate model\n",
    "\n",
    "        Parameters:\n",
    "        - grid_rows, grid_cols: Dimensions of the grid (e.g., 16x12) will be changed to assist with model speed and resolution.\n",
    "        - et_coeff: Paraemeter to control water loss per step..\n",
    "        - optimal_low, optimal_high: Bounds for optimal water level, outside of bounds pentalties start\n",
    "        - stages_duration: Time steps per growth stage.\n",
    "        - end_on_death: If True, simulation ends early if any crop dies. (should assist with RL training speeds)\n",
    "        - diffusion: If True, enable water diffusion between cells (not implemented yet).\n",
    "        - verbose: If True, shows plots of at each stage change\n",
    "        \"\"\"\n",
    "\n",
    "        #User Specicied Variables\n",
    "        self.rows = grid_rows\n",
    "        self.cols = grid_cols\n",
    "        self.optimal_low = optimal_low\n",
    "        self.optimal_high = optimal_high\n",
    "        self.stages_duration = stages_duration\n",
    "        self.total_steps = stages_duration * 4\n",
    "        self.end_on_death = end_on_death\n",
    "        self.diffusion = diffusion\n",
    "        self.et_coeff = np.full((self.rows, self.cols), et_coeff)\n",
    "        self.rand_et = rand_et\n",
    "        self.verbose = verbose\n",
    "        #Modify ET to add some variability\n",
    "        if self.rand_et:\n",
    "            self.et_random_factor = np.clip(np.random.normal(1.0, 0.1, (self.rows, self.cols)), 0.8, 1.2)\n",
    "        else:\n",
    "            self.et_random_factor = np.ones((self.rows, self.cols))\n",
    "\n",
    "        #Model Variables\n",
    "        self.stages = ['Bud Break', 'Flowering', 'Fruit Set', 'Ripening', 'Harvest']\n",
    "        self.max_water = 1 #Normalized max quantity of water soil can absorb\n",
    "        self.current_step = 0\n",
    "        self.current_stage = 0\n",
    "        self.time_in_stage = np.zeros((self.rows, self.cols), dtype=int)  # Time steps in current stage per cell\n",
    "        self.alive = np.ones((self.rows, self.cols), dtype=bool)  # Alive/dead per cell\n",
    "        self.water = np.full((self.rows, self.cols), 0.5)  # Initial even water level\n",
    "        self.quantity_grid = np.zeros((self.rows, self.cols))  # Per-cell quantity\n",
    "        self.quality_grid = np.ones((self.rows, self.cols))  # Per-cell quality\n",
    "        self.total_water_usage = 0.0  # Total irrigation water used (inch-acres)\n",
    "        self.cum_rain = 0.0\n",
    "\n",
    "        #Turnable stage parameters (tune to model realistic grape growth across different stages)\n",
    "        self.stage_params = {\n",
    "            0: {'qty_rate': 0.00005, 'under_penalty': -0.05, 'over_penalty': 0.1, 'qual_factor': 0.001},\n",
    "            1: {'qty_rate': 0.0001, 'under_penalty': -0.1, 'over_penalty': 0.5, 'qual_factor': 0.002},\n",
    "            2: {'qty_rate': 0.0003, 'under_penalty': -0.9, 'over_penalty': 3.5, 'qual_factor': 0.005},\n",
    "            3: {'qty_rate': 0.00015, 'under_penalty': -0.5, 'over_penalty': 0.5, 'qual_factor': 0.015}\n",
    "        }\n",
    "\n",
    "        #Store simulation history for graphing\n",
    "        self.water_history = []\n",
    "        self.quantity_history = []\n",
    "        self.quality_history = []\n",
    "        self.usage_history = []\n",
    "        self.rain_history = []  # Per-step rain\n",
    "        self.cum_rain_history = []  # Cumulative rain over steps\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the simulation to initial state.\"\"\"\n",
    "        self.__init__(grid_rows=self.rows, grid_cols=self.cols, et_coeff=self.et_coeff,\n",
    "                      optimal_low=self.optimal_low, optimal_high=self.optimal_high,\n",
    "                      stages_duration=self.stages_duration, end_on_death=self.end_on_death,\n",
    "                      diffusion=self.diffusion, rand_et=self.rand_et)\n",
    "\n",
    "    def step(self, rainfall=0.0, irrigation=None):\n",
    "        \"\"\"\n",
    "        Advance the simulation by one time step.\n",
    "\n",
    "        Parameters:\n",
    "        - rainfall: Uniform rainfall amount (inches) added to all cells.\n",
    "        - irrigation: 2D array of irrigation amounts per cell (inches).\n",
    "\n",
    "        Returns:\n",
    "        - done: True if simulation ended (200 steps or death if enabled).\n",
    "        - info: Dict with current quantity, quality, water_usage.\n",
    "        \"\"\"\n",
    "\n",
    "        #Check if current step is past total step limit\n",
    "        if self.current_step >= self.total_steps:\n",
    "            return True, self.get_info()\n",
    "\n",
    "        if irrigation is None:\n",
    "            irrigation = np.zeros((self.rows, self.cols))\n",
    "\n",
    "        #Add rainfall and irrigation\n",
    "        self.water += rainfall/6\n",
    "        self.water += irrigation/6\n",
    "        self.total_water_usage += np.sum(irrigation) / (self.rows * self.cols)  # Average inch-acres\n",
    "        self.cum_rain += rainfall\n",
    "\n",
    "        #Check for crop death due to water\n",
    "        over_water = self.water > self.max_water #Death by drowning\n",
    "        under_water = self.water < 0.01  #Death by drought\n",
    "        dead_cells = over_water | under_water\n",
    "\n",
    "        #Set cells to dead and set quantity and quality to 0\n",
    "        self.alive[dead_cells] = False\n",
    "        #self.quantity_grid[dead_cells] = 0.0\n",
    "        self.quality_grid[dead_cells] = 0.0\n",
    "\n",
    "        #Check for dead cells and end on death\n",
    "        if self.end_on_death and np.any(~self.alive):\n",
    "            return True, self.get_info()\n",
    "\n",
    "        #Evapotranspiration loss\n",
    "        et_loss = self.et_coeff * np.sqrt(self.water) * self.et_random_factor #Simple water loss per step (more water = larger loss)\n",
    "        self.water = np.maximum(self.water - et_loss, 0.0)\n",
    "\n",
    "        #Diffusion (spread water to adjacent cells [WIP])\n",
    "        if self.diffusion:\n",
    "            pass  #Not implimented yet\n",
    "\n",
    "        # Update growth\n",
    "        self.update_growth()\n",
    "\n",
    "        # Advance time\n",
    "        self.current_step += 1\n",
    "        self.time_in_stage += 1\n",
    "\n",
    "        # Check stage transition\n",
    "        if self.time_in_stage[0,0] >= self.stages_duration:\n",
    "            self.current_stage += 1\n",
    "            self.time_in_stage.fill(0)\n",
    "\n",
    "            if self.verbose:\n",
    "              self.print_stage_graphs()\n",
    "\n",
    "        # Record history\n",
    "        self.water_history.append(np.mean(self.water))\n",
    "        self.quantity_history.append(np.sum(self.quantity_grid))\n",
    "        self.quality_history.append(np.mean(self.quality_grid))\n",
    "        self.usage_history.append(self.total_water_usage)\n",
    "\n",
    "        done = self.current_step >= self.total_steps\n",
    "        return done, self.get_info()\n",
    "\n",
    "    def update_growth(self):\n",
    "        \"\"\"Update quantity and quality based on current water and stage.\"\"\"\n",
    "\n",
    "        #get parameters based on growth stage\n",
    "        params = self.stage_params[self.current_stage]\n",
    "        alive_mask = self.alive.astype(float)\n",
    "\n",
    "        #Quantity change\n",
    "        under = self.water < self.optimal_low\n",
    "        over = self.water > self.optimal_high\n",
    "\n",
    "        qty_deltas = (params['qty_rate'] * (1 + params['under_penalty'] * under.astype(float) + params['over_penalty'] * over.astype(float)))\n",
    "        self.quantity_grid += qty_deltas * alive_mask\n",
    "\n",
    "        #Quality change\n",
    "\n",
    "        #Water penalty\n",
    "        dev_under = np.maximum(self.optimal_low - self.water, 0)\n",
    "        dev_over = np.maximum(self.water - self.optimal_high, 0)\n",
    "        water_penalties = params['qual_factor'] * (dev_under + dev_over)\n",
    "\n",
    "        #Quantity penalty\n",
    "        total_quantity = np.sum(self.quantity_grid)\n",
    "        qty_penalty = 0\n",
    "        if total_quantity > 5:\n",
    "            qty_penalty = params['qual_factor']* 0.1 * (total_quantity - 5)\n",
    "\n",
    "        self.quality_grid -= (water_penalties + qty_penalty) * alive_mask\n",
    "        self.quality_grid = np.maximum(self.quality_grid, 0)\n",
    "\n",
    "    def get_info(self):\n",
    "        \"\"\"Get current simulation info.\"\"\"\n",
    "        return {\n",
    "            'quantity': np.sum(self.quantity_grid),\n",
    "            'quality': np.mean(self.quality_grid),\n",
    "            'water_usage': self.total_water_usage,\n",
    "            'stage': self.stages[self.current_stage],\n",
    "            'step': self.current_step,\n",
    "            'any_dead': np.any(~self.alive)}\n",
    "\n",
    "    def print_stage_graphs(self):\n",
    "        \"\"\"Print field-wide graphs every stage change and return image.\"\"\"\n",
    "\n",
    "        print(f\"Stage {self.stages[self.current_stage - 1]} completed at step {self.current_step}\")\n",
    "        print(\"Average ET Coefficient:\", np.mean(self.et_coeff))\n",
    "        print(\"Average Water Level:\", np.mean(self.water))\n",
    "        print(\"Crop State: Quantity={:.2f}, Quality={:.2f}\".format(np.sum(self.quantity_grid), np.mean(self.quality_grid)))\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        # Water Grid\n",
    "        im0 = axs[0].imshow(self.water, cmap='Blues', vmin=0, vmax=1)\n",
    "        axs[0].set_title('Water Grid')\n",
    "        fig.colorbar(im0, ax=axs[0])\n",
    "\n",
    "        # Growth Grid: Use -1 for dead cells and set under-color to black\n",
    "        growth_vis = self.quantity_grid.copy()\n",
    "        growth_vis[~self.alive] = -1\n",
    "        cmap = plt.get_cmap('Greens').copy()\n",
    "        cmap.set_under(color='black')\n",
    "        im1 = axs[1].imshow(growth_vis, cmap=cmap, vmin=0, vmax=3.0)\n",
    "        axs[1].set_title('Growth Grid (Black=Dead)')\n",
    "        fig.colorbar(im1, ax=axs[1])\n",
    "\n",
    "        # Quality Grid\n",
    "        im2 = axs[2].imshow(self.quality_grid, cmap='Oranges', vmin=0, vmax=1)\n",
    "        axs[2].set_title('Quality Grid')\n",
    "        fig.colorbar(im2, ax=axs[2])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "        plt.show(fig)\n",
    "        plt.close('all')\n",
    "\n",
    "    def plot_histories(self):\n",
    "        \"\"\"Plot simulation histories.\"\"\"\n",
    "        steps = range(len(self.water_history))\n",
    "        fig, axs = plt.subplots(6, 1, figsize=(10, 18))\n",
    "\n",
    "        axs[0].plot(steps, self.water_history)\n",
    "        axs[0].set_title('Average Water Level Over Time')\n",
    "\n",
    "        axs[1].plot(steps, self.quantity_history)\n",
    "        axs[1].set_title('Crop Quantity Over Time')\n",
    "\n",
    "        axs[2].plot(steps, self.quality_history)\n",
    "        axs[2].set_title('Crop Quality Over Time')\n",
    "\n",
    "        axs[3].plot(steps, self.usage_history)\n",
    "        axs[3].set_title('Total Water Usage Over Time')\n",
    "\n",
    "        axs[4].step(steps, self.rain_history)\n",
    "        axs[4].set_title('Rain Amount Per Step')\n",
    "\n",
    "        axs[5].plot(steps, np.array(self.cum_rain_history) + np.array(self.usage_history))\n",
    "        axs[5].set_title('Total Added Water (Rain + Irrigation) Over Time')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8d9d9d-9644-49bb-925e-fcb269cbea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Agents\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_sizes):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        sizes = [input_dim] + hidden_sizes\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "        self.hidden_layers = nn.ModuleList(layers)\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = obs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        logits = self.output_layer(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return probs\n",
    "\n",
    "class CentralizedIrrigationModel:\n",
    "    def __init__(self, state_dim, lr, gamma):\n",
    "        self.model = PolicyNet(input_dim=state_dim, output_dim=2, hidden_sizes=[64, 64])  # 0=no irrigate, 1=yes\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32)\n",
    "        probs = self.model(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        G = 0.0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.append(G)\n",
    "        returns.reverse()\n",
    "        return returns\n",
    "\n",
    "    def update_model(self, log_probs, rewards):\n",
    "        returns = torch.tensor(self.compute_returns(rewards), dtype=torch.float32)\n",
    "        loss = sum(-log_p * G for log_p, G in zip(log_probs, returns))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "class LocalIrrigationModel:\n",
    "    def __init__(self, state_dim, lr, gamma):\n",
    "        self.model = PolicyNet(input_dim=state_dim, output_dim=4, hidden_sizes=[64, 64])  # 0-3 map to [0.25, 0.5, 0.75, 1.0]\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32)\n",
    "        probs = self.model(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        G = 0.0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.append(G)\n",
    "        returns.reverse()\n",
    "        return returns\n",
    "\n",
    "    def update_model(self, log_probs, rewards):\n",
    "        returns = torch.tensor(self.compute_returns(rewards), dtype=torch.float32)\n",
    "        loss = sum(-log_p * G for log_p, G in zip(log_probs, returns))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "def get_global_state(model, time_since_last_irrigate, forecasted_rain=0.0):\n",
    "    \"\"\"Global state for centralized agent.\"\"\"\n",
    "    mean_water = np.mean(model.water)\n",
    "    norm_time_since_irr = time_since_last_irrigate / model.total_steps  # Normalized [0,1]\n",
    "    return np.array([mean_water, norm_time_since_irr, forecasted_rain])\n",
    "\n",
    "def get_local_state(model, region_idx, total_irrigation_local, projected_rain=0.0, num_locals=4):\n",
    "    \"\"\"Local state for a subgrid.\"\"\"\n",
    "    rows_per_local = model.rows // int(np.sqrt(num_locals))\n",
    "    cols_per_local = model.cols // int(np.sqrt(num_locals))\n",
    "    row_idx = region_idx // int(np.sqrt(num_locals))\n",
    "    col_idx = region_idx % int(np.sqrt(num_locals))\n",
    "    row_start = row_idx * rows_per_local\n",
    "    row_end = row_start + rows_per_local\n",
    "    col_start = col_idx * cols_per_local\n",
    "    col_end = col_start + cols_per_local\n",
    "\n",
    "    sub_water = model.water[row_start:row_end, col_start:col_end]\n",
    "    sub_alive = model.alive[row_start:row_end, col_start:col_end]\n",
    "\n",
    "    mean_water = np.mean(sub_water)\n",
    "    norm_total_irr = total_irrigation_local / model.total_steps  # Normalized\n",
    "    mean_health = np.mean(sub_alive)\n",
    "    return np.array([mean_water, norm_total_irr, projected_rain, mean_health]), (row_start, row_end, col_start, col_end)\n",
    "\n",
    "def save_models(central, local_agents, path='models/'):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(central.model.state_dict(), f\"{path}central.pth\")\n",
    "    for i, agent in enumerate(local_agents):\n",
    "        torch.save(agent.model.state_dict(), f\"{path}local_{i}.pth\")\n",
    "\n",
    "def load_agents(global_state_dim, local_state_dim, lr, gamma, num_locals, path='models/'):\n",
    "    central = CentralizedIrrigationModel(global_state_dim, lr, gamma)\n",
    "    central.model.load_state_dict(torch.load(f\"{path}central.pth\"))\n",
    "\n",
    "    local_agents = []\n",
    "    for i in range(num_locals):\n",
    "        local = LocalIrrigationModel(local_state_dim, lr, gamma)\n",
    "        local.model.load_state_dict(torch.load(f\"{path}local_{i}.pth\"))\n",
    "        local_agents.append(local)\n",
    "\n",
    "    return central, local_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ebf375-dbdf-44d3-b9ff-b1deea7cf64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agents(alpha_quantity=1.0, alpha_quality=1.0, alpha_steps=5.0, beta=0.1, num_episodes=1000, lr=0.001, gamma=1, num_locals=4, end_on_death=True, rand_et=True, rain_chance=0.05, rain_amounts=[0.25, 0.5, 0.75, 1.0], rain_probs=[0.45, 0.3, 0.15, 0.1], save_path=\"./models\"):\n",
    "    env = VineyardSurrogate(end_on_death=end_on_death, rand_et=rand_et)  # Enable early end if desired\n",
    "    global_state_dim = 3  # mean_water, time_since_last_irr, forecasted_rain\n",
    "    local_state_dim = 4  # mean_water, total_irr_local, projected_rain, mean_health\n",
    "\n",
    "    central_agent = CentralizedIrrigationModel(global_state_dim, lr, gamma)\n",
    "    local_agents = [LocalIrrigationModel(local_state_dim, lr, gamma) for _ in range(num_locals)]\n",
    "\n",
    "    irrigation_amounts = np.array([0.25, 0.5, 0.75, 1.0])  # For locals when active\n",
    "\n",
    "    episode_rewards = []  # To track progress\n",
    "    episode_durations = []  # To track simulation timesteps per episode\n",
    "\n",
    "    # For plotting\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    plt.ion()  # Interactive mode for live updates in Jupyter\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        rewards = []\n",
    "        central_log_probs = []\n",
    "        central_steps = []  # Timesteps where central acted (all)\n",
    "\n",
    "        locals_log_probs = [[] for _ in range(num_locals)]\n",
    "        locals_steps = [[] for _ in range(num_locals)]  # Timesteps where each local acted\n",
    "\n",
    "        time_since_last_irrigate = 0\n",
    "        locals_total_irr = [0.0 for _ in range(num_locals)]  # Cumulative per local\n",
    "\n",
    "        # Tracking histories (per episode)\n",
    "        central_irrigate_history = []  # 1=yes, 0=no per step\n",
    "        locals_irrigation_history = [[] for _ in range(num_locals)]  # Cumulative per region per step\n",
    "\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            # Sample rain for this step\n",
    "            if np.random.rand() < rain_chance:\n",
    "                rainfall = np.random.choice(rain_amounts, p=rain_probs)\n",
    "            else:\n",
    "                rainfall = 0.0\n",
    "\n",
    "            global_state = get_global_state(env, time_since_last_irrigate, forecasted_rain=rainfall)\n",
    "            irrigate, central_log = central_agent.select_action(global_state)\n",
    "            central_log_probs.append(central_log)\n",
    "            central_steps.append(step)\n",
    "            central_irrigate_history.append(irrigate)\n",
    "\n",
    "            irrigation_grid = np.zeros((env.rows, env.cols))\n",
    "\n",
    "            if irrigate == 1:  # Yes: Activate locals\n",
    "                time_since_last_irrigate = 0  # Reset counter\n",
    "                for i in range(num_locals):\n",
    "                    local_state, sub_slice = get_local_state(env, i, locals_total_irr[i], projected_rain=rainfall, num_locals=num_locals)\n",
    "                    amount_idx, local_log = local_agents[i].select_action(local_state)\n",
    "                    locals_log_probs[i].append(local_log)\n",
    "                    locals_steps[i].append(step)\n",
    "\n",
    "                    amount = irrigation_amounts[amount_idx]\n",
    "                    r_start, r_end, c_start, c_end = sub_slice\n",
    "                    irrigation_grid[r_start:r_end, c_start:c_end] = amount\n",
    "\n",
    "                    locals_total_irr[i] += amount * ((r_end - r_start) * (c_end - c_start)) / (env.rows * env.cols)  # Scaled addition\n",
    "            else:\n",
    "                time_since_last_irrigate += 1\n",
    "\n",
    "            # Update local histories (cumulative even if no action)\n",
    "            for i in range(num_locals):\n",
    "                locals_irrigation_history[i].append(locals_total_irr[i])\n",
    "\n",
    "            done, info = env.step(rainfall=rainfall, irrigation=irrigation_grid)\n",
    "            #Reward for keeping crops alive\n",
    "            alive_fraction = np.mean(env.alive)\n",
    "            reward = - (1 - alive_fraction)  # Dense: -1 if all dead, 0 if all alive\n",
    "            if done:\n",
    "                reward += alpha_quantity * info['quantity'] + alpha_quality * info['quality'] - beta * info['water_usage']  + alpha_steps * env.current_step\n",
    "            rewards.append(reward)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        episode_rewards.append(sum(rewards))\n",
    "        episode_durations.append(env.current_step)\n",
    "\n",
    "        # Update central (full episode)\n",
    "        central_agent.update_model(central_log_probs, rewards)\n",
    "\n",
    "        # Update locals (only if they acted; compute returns from their action steps onward)\n",
    "        for i in range(num_locals):\n",
    "            if locals_log_probs[i]:\n",
    "                local_returns = []\n",
    "                for act_step in locals_steps[i]:\n",
    "                    G = 0.0\n",
    "                    for r in rewards[act_step:]:\n",
    "                        G = r + gamma * G\n",
    "                    local_returns.append(G)\n",
    "                local_agents[i].update_model(locals_log_probs[i], local_returns)\n",
    "\n",
    "        if episode % 1 == 0:\n",
    "            print(f\"Episode {episode}: Reward = {episode_rewards[-1]}, Quantity={info['quantity']}, Quality={info['quality']}, Usage={info['water_usage']}, Early End={env.current_step < env.total_steps}\")\n",
    "\n",
    "        # Live plot updates every episode\n",
    "        episodes = np.arange(episode + 1)\n",
    "\n",
    "        # Duration plot\n",
    "        axs[0].cla()\n",
    "        axs[0].plot(episodes, episode_durations, label='Raw Duration', color='blue')\n",
    "        if episode >= 9:\n",
    "            moving_avg_dur = np.convolve(episode_durations, np.ones(10)/10, mode='valid')\n",
    "            axs[0].plot(episodes[9:], moving_avg_dur, label='10-Ep Avg', color='red')\n",
    "        axs[0].set_title('Simulation Duration vs Episode')\n",
    "        axs[0].set_xlabel('Episode')\n",
    "        axs[0].set_ylabel('Timesteps')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Reward plot\n",
    "        axs[1].cla()\n",
    "        axs[1].plot(episodes, episode_rewards, label='Raw Reward', color='green')\n",
    "        if episode >= 9:\n",
    "            moving_avg_rew = np.convolve(episode_rewards, np.ones(10)/10, mode='valid')\n",
    "            axs[1].plot(episodes[9:], moving_avg_rew, label='10-Ep Avg', color='orange')\n",
    "        axs[1].set_title('Reward vs Episode')\n",
    "        axs[1].set_xlabel('Episode')\n",
    "        axs[1].set_ylabel('Reward')\n",
    "        axs[1].legend()\n",
    "\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    # Final plots (same as live, but static)\n",
    "    plt.ioff()\n",
    "\n",
    "    # Plot decision tracking for last episode (pad to 200 steps if early end)\n",
    "    max_steps = env.total_steps\n",
    "    steps = np.arange(max_steps)\n",
    "\n",
    "    # Pad histories\n",
    "    padded_central = np.pad(central_irrigate_history, (0, max_steps - len(central_irrigate_history)), mode='constant', constant_values=central_irrigate_history[-1] if central_irrigate_history else 0)\n",
    "    padded_locals = []\n",
    "    for hist in locals_irrigation_history:\n",
    "        padded = np.pad(hist, (0, max_steps - len(hist)), mode='constant', constant_values=hist[-1] if hist else 0)\n",
    "        padded_locals.append(padded)\n",
    "\n",
    "    fig_dec, axs_dec = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    axs_dec[0].step(steps, padded_central, where='post')\n",
    "    axs_dec[0].set_title('Central Agent: Irrigate Decisions Over Time')\n",
    "    axs_dec[0].set_xlabel('Step')\n",
    "    axs_dec[0].set_ylabel('Irrigate (1=Yes, 0=No)')\n",
    "    axs_dec[0].set_xlim(0, max_steps)\n",
    "\n",
    "    for i in range(num_locals):\n",
    "        axs_dec[1].plot(steps, padded_locals[i], label=f'Region {i}')\n",
    "    axs_dec[1].set_title('Local Agents: Cumulative Irrigation Over Time')\n",
    "    axs_dec[1].set_xlabel('Step')\n",
    "    axs_dec[1].set_ylabel('Cumulative Irrigation')\n",
    "    axs_dec[1].legend()\n",
    "    axs_dec[1].set_xlim(0, max_steps)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if save_path:\n",
    "        save_models(central_agent, local_agents, save_path)\n",
    "\n",
    "    return central_agent, local_agents, episode_rewards, episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d06e17b-2d27-449e-9308-a3fcf8235047",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m central, locals_list, rewards_history, episode_durations \u001b[38;5;241m=\u001b[39m train_agents(alpha_quantity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, alpha_quality\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, alpha_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_locals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, end_on_death\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rand_et\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rain_chance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, rain_amounts\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m1.0\u001b[39m], rain_probs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.45\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.1\u001b[39m], save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_agents' is not defined"
     ]
    }
   ],
   "source": [
    "central, locals_list, rewards_history, episode_durations = train_agents(alpha_quantity=1.0, alpha_quality=1.0, alpha_steps=5.0, beta=0.1, num_episodes=1000, lr=0.001, gamma=1, num_locals=4, end_on_death=True, rand_et=True, rain_chance=0.05, rain_amounts=[0.25, 0.5, 0.75, 1.0], rain_probs=[0.45, 0.3, 0.15, 0.1], save_path=\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8e37e-b355-4499-8d62-f3c04fa3a245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
